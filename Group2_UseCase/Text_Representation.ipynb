{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b6f683-9159-4963-a0c6-6bac5a2ed34d",
   "metadata": {},
   "source": [
    "## Text Representation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5c58a-b007-4d78-bac9-accd572412a8",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a121c75-2afd-4dbf-a4b8-ed6bbf7dcf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7359435-f97d-49ec-b418-2852609f98ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer for converting text data into a bag-of-words representation.\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b2dad61-0cba-4fd1-9f4a-a21ebc713821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['aand' 'abandon' 'abandoning' ... 'zvrzahl' 'zürich' 'ánd']\n",
      "Message Vector Shape: (2254, 11443)\n",
      "Message Vector Array:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Ensure all entries in the 'Content' column are strings.\n",
    "df['Content'] = df['Content'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "\n",
    "# Fill missing values in the 'Content' column with empty strings.\n",
    "df['Content'] = df['Content'].fillna('')\n",
    "\n",
    "# Initialize the CountVectorizer for bag-of-words representation.\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the text in the 'Content' column into a sparse matrix of token counts.\n",
    "message_vector = count_vectorizer.fit_transform(df['Content'])\n",
    "message_vector\n",
    "\n",
    "# Print results for verification.\n",
    "print(\"Feature Names:\", count_vectorizer.get_feature_names_out())  # Vocabulary of the vectorizer.\n",
    "print(\"Message Vector Shape:\", message_vector.shape)  # Shape of the resulting sparse matrix.\n",
    "print(\"Message Vector Array:\\n\", message_vector.toarray())  # Dense array representation of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f036b2-9ed5-4791-8cc1-044de12ef9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      aand  abandon  abandoning  abe  aber  abilities  ability  abillity  \\\n",
      "0        0        0           0    0     0          0        0         0   \n",
      "1        0        0           0    0     0          0        0         0   \n",
      "2        0        0           0    0     0          0        0         0   \n",
      "3        0        0           0    0     0          0        0         0   \n",
      "4        0        0           0    0     0          0        0         0   \n",
      "...    ...      ...         ...  ...   ...        ...      ...       ...   \n",
      "2249     0        0           0    0     0          0        0         0   \n",
      "2250     0        0           0    0     0          0        0         0   \n",
      "2251     0        0           0    0     0          0        0         0   \n",
      "2252     0        0           0    0     0          0        0         0   \n",
      "2253     0        0           0    0     0          0        0         0   \n",
      "\n",
      "      abilty  able  ...  youyour  youyours  zealand  zero  zone  zurich  \\\n",
      "0          0     1  ...        0         0        0     0     0       0   \n",
      "1          0     0  ...        0         0        0     0     0       0   \n",
      "2          0     0  ...        0         0        0     0     0       0   \n",
      "3          0     0  ...        0         0        0     0     0       0   \n",
      "4          0     0  ...        0         0        0     0     0       0   \n",
      "...      ...   ...  ...      ...       ...      ...   ...   ...     ...   \n",
      "2249       0     0  ...        0         0        0     0     0       0   \n",
      "2250       0     0  ...        0         0        0     0     0       0   \n",
      "2251       0     0  ...        0         0        0     0     0       0   \n",
      "2252       0     0  ...        0         0        0     0     0       0   \n",
      "2253       0     0  ...        0         0        0     0     0       0   \n",
      "\n",
      "      zurick  zvrzahl  zürich  ánd  \n",
      "0          0        0       0    0  \n",
      "1          0        0       0    0  \n",
      "2          0        0       0    0  \n",
      "3          0        0       0    0  \n",
      "4          0        0       0    0  \n",
      "...      ...      ...     ...  ...  \n",
      "2249       0        0       1    0  \n",
      "2250       0        0       0    0  \n",
      "2251       0        0       0    0  \n",
      "2252       0        0       0    0  \n",
      "2253       0        0       0    0  \n",
      "\n",
      "[2254 rows x 11443 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the sparse matrix to a dense array.\n",
    "message_array = message_vector.toarray()\n",
    "\n",
    "# Create a DataFrame from the dense array, using feature names as column headers.\n",
    "df_countvectorizer = pd.DataFrame(data=message_array, columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df_countvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e39bf3c-b257-47eb-a09b-dc9dfdbe4f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aand', 'abandon', 'abandoning', 'abe', 'aber', 'abilities', 'ability',\n",
       "       'abillity', 'abilty', 'able',\n",
       "       ...\n",
       "       'youyour', 'youyours', 'zealand', 'zero', 'zone', 'zurich', 'zurick',\n",
       "       'zvrzahl', 'zürich', 'ánd'],\n",
       "      dtype='object', length=11443)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the column names of the DataFrame created from the CountVectorizer output.\n",
    "df_countvectorizer.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98725de7-6568-4dc7-9d29-00ade742092c",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c29d5f-1577-4c51-9d5e-fd6e8c6531b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer for converting text data into a TF-IDF representation.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c739e1e2-7aa0-4e92-a533-a68e8a45eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all entries in 'Content' are strings.\n",
    "df['Content'] = df['Content'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "\n",
    "# Define a function to remove the first `n` words from a text.\n",
    "def remove_first_words(text, n=3):\n",
    "    words = text.split()  # Split text into words.\n",
    "    return \" \".join(words[n:])  # Join words after skipping the first `n`.\n",
    "\n",
    "# Create a new column with the first 3 words removed from 'Content'.\n",
    "df['Filtered_Content'] = df['Content'].apply(lambda x: remove_first_words(x, n=3))\n",
    "\n",
    "# Initialize the TfidfVectorizer for TF-IDF representation.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer and transform the 'Filtered_Content' column into a TF-IDF matrix.\n",
    "tfidf_vectorizer.fit(df['Filtered_Content'])\n",
    "tfidf_message_vector = tfidf_vectorizer.transform(df['Filtered_Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc251a0-ee07-45a5-a4ab-ed00e569a145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2254, 11299)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the shape of the TF-IDF matrix (rows represent number of documents and columns represent number of unique terms).\n",
    "tfidf_message_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42c4ed40-43a2-41e8-bd2f-d544b2738785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      aand  abandon  abandoning  abe  aber  abilities  ability  abillity  \\\n",
      "0      0.0      0.0         0.0  0.0   0.0        0.0      0.0       0.0   \n",
      "1      0.0      0.0         0.0  0.0   0.0        0.0      0.0       0.0   \n",
      "2      0.0      0.0         0.0  0.0   0.0        0.0      0.0       0.0   \n",
      "3      0.0      0.0         0.0  0.0   0.0        0.0      0.0       0.0   \n",
      "4      0.0      0.0         0.0  0.0   0.0        0.0      0.0       0.0   \n",
      "...    ...      ...         ...  ...   ...        ...      ...       ...   \n",
      "2249   0.0      0.0         0.0  0.0   0.0        0.0      0.0       0.0   \n",
      "2250   0.0      0.0         0.0  0.0   0.0        0.0      0.0       0.0   \n",
      "2251   0.0      0.0         0.0  0.0   0.0        0.0      0.0       0.0   \n",
      "2252   0.0      0.0         0.0  0.0   0.0        0.0      0.0       0.0   \n",
      "2253   0.0      0.0         0.0  0.0   0.0        0.0      0.0       0.0   \n",
      "\n",
      "      abilty      able  ...  youyour  youyours  zealand  zero  zone  zurich  \\\n",
      "0        0.0  0.056007  ...      0.0       0.0      0.0   0.0   0.0     0.0   \n",
      "1        0.0  0.000000  ...      0.0       0.0      0.0   0.0   0.0     0.0   \n",
      "2        0.0  0.000000  ...      0.0       0.0      0.0   0.0   0.0     0.0   \n",
      "3        0.0  0.000000  ...      0.0       0.0      0.0   0.0   0.0     0.0   \n",
      "4        0.0  0.000000  ...      0.0       0.0      0.0   0.0   0.0     0.0   \n",
      "...      ...       ...  ...      ...       ...      ...   ...   ...     ...   \n",
      "2249     0.0  0.000000  ...      0.0       0.0      0.0   0.0   0.0     0.0   \n",
      "2250     0.0  0.000000  ...      0.0       0.0      0.0   0.0   0.0     0.0   \n",
      "2251     0.0  0.000000  ...      0.0       0.0      0.0   0.0   0.0     0.0   \n",
      "2252     0.0  0.000000  ...      0.0       0.0      0.0   0.0   0.0     0.0   \n",
      "2253     0.0  0.000000  ...      0.0       0.0      0.0   0.0   0.0     0.0   \n",
      "\n",
      "      zurick  zvrzahl    zürich  ánd  \n",
      "0        0.0      0.0  0.000000  0.0  \n",
      "1        0.0      0.0  0.000000  0.0  \n",
      "2        0.0      0.0  0.000000  0.0  \n",
      "3        0.0      0.0  0.000000  0.0  \n",
      "4        0.0      0.0  0.000000  0.0  \n",
      "...      ...      ...       ...  ...  \n",
      "2249     0.0      0.0  0.136186  0.0  \n",
      "2250     0.0      0.0  0.000000  0.0  \n",
      "2251     0.0      0.0  0.000000  0.0  \n",
      "2252     0.0      0.0  0.000000  0.0  \n",
      "2253     0.0      0.0  0.000000  0.0  \n",
      "\n",
      "[2254 rows x 11299 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the TF-IDF sparse matrix to a dense array.\n",
    "message_tfidf_array = tfidf_message_vector.toarray()\n",
    "\n",
    "# Create a DataFrame from the TF-IDF array with feature names as column headers.\n",
    "df_tfidf = pd.DataFrame(data=message_tfidf_array, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd5f8481-b502-4090-932b-f657e9d6515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7871396895787139\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.14       104\n",
      "           1       0.78      1.00      0.88       347\n",
      "\n",
      "    accuracy                           0.79       451\n",
      "   macro avg       0.89      0.54      0.51       451\n",
      "weighted avg       0.83      0.79      0.71       451\n",
      "\n",
      "Top 50 words in order of importance:\n",
      "Word: kramer, Importance: 0.01150\n",
      "Word: folklore, Importance: 0.00998\n",
      "Word: russian, Importance: 0.00822\n",
      "Word: swiss, Importance: 0.00820\n",
      "Word: reject, Importance: 0.00638\n",
      "Word: constance, Importance: 0.00605\n",
      "Word: price, Importance: 0.00527\n",
      "Word: sorry, Importance: 0.00498\n",
      "Word: week, Importance: 0.00448\n",
      "Word: delivery, Importance: 0.00406\n",
      "Word: ltd, Importance: 0.00382\n",
      "Word: already, Importance: 0.00373\n",
      "Word: best, Importance: 0.00365\n",
      "Word: traditional, Importance: 0.00364\n",
      "Word: final, Importance: 0.00360\n",
      "Word: offer, Importance: 0.00359\n",
      "Word: willing, Importance: 0.00358\n",
      "Word: issues, Importance: 0.00347\n",
      "Word: years, Importance: 0.00332\n",
      "Word: parking, Importance: 0.00322\n",
      "Word: opening, Importance: 0.00322\n",
      "Word: regardsalex, Importance: 0.00322\n",
      "Word: chris, Importance: 0.00318\n",
      "Word: meyer, Importance: 0.00315\n",
      "Word: cuisine, Importance: 0.00306\n",
      "Word: corporate, Importance: 0.00306\n",
      "Word: conditions, Importance: 0.00300\n",
      "Word: kim, Importance: 0.00299\n",
      "Word: international, Importance: 0.00298\n",
      "Word: accept, Importance: 0.00293\n",
      "Word: alex, Importance: 0.00288\n",
      "Word: regards, Importance: 0.00287\n",
      "Word: first, Importance: 0.00282\n",
      "Word: sponsorship, Importance: 0.00279\n",
      "Word: vaccine, Importance: 0.00278\n",
      "Word: future, Importance: 0.00272\n",
      "Word: catering, Importance: 0.00264\n",
      "Word: make, Importance: 0.00261\n",
      "Word: students, Importance: 0.00261\n",
      "Word: think, Importance: 0.00254\n",
      "Word: another, Importance: 0.00247\n",
      "Word: music, Importance: 0.00245\n",
      "Word: event, Importance: 0.00244\n",
      "Word: buffet, Importance: 0.00240\n",
      "Word: sponsors, Importance: 0.00239\n",
      "Word: isa, Importance: 0.00238\n",
      "Word: could, Importance: 0.00237\n",
      "Word: also, Importance: 0.00236\n",
      "Word: city, Importance: 0.00235\n",
      "Word: kind, Importance: 0.00234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the labels from the CSV file.\n",
    "labels = pd.read_csv('df.csv')  # Assumes 'NegoOutcomeLabel' exists in the file.\n",
    "\n",
    "# Add the target variable to the TF-IDF DataFrame.\n",
    "df_tfidf['NegoOutcomeLabel'] = labels['NegoOutcomeLabel']\n",
    "\n",
    "# Split the data into features (X) and target (y).\n",
    "X = df_tfidf.drop(columns=['NegoOutcomeLabel'])  # Features (TF-IDF matrix).\n",
    "y = df_tfidf['NegoOutcomeLabel']  # Target variable.\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optional: Apply PCA to reduce dimensionality (e.g., for model efficiency).\n",
    "pca = PCA(n_components=100)  # Reduce to 100 dimensions.\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train a RandomForestClassifier on the PCA-transformed data.\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test data.\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model.\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# **Feature Analysis Without PCA**\n",
    "# Train the model on the original TF-IDF data.\n",
    "clf_feature_importance = RandomForestClassifier(random_state=42)\n",
    "clf_feature_importance.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importances and link them with feature names.\n",
    "feature_importances = clf_feature_importance.feature_importances_\n",
    "feature_names = X.columns  # Names of the TF-IDF features.\n",
    "\n",
    "# Combine feature importances with their corresponding terms.\n",
    "important_features = list(zip(feature_importances, feature_names))\n",
    "\n",
    "# Sort features by importance in descending order.\n",
    "important_features_sorted = sorted(important_features, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Extract the top 50 most important features.\n",
    "top_50_features = important_features_sorted[:50]\n",
    "\n",
    "# Display the top 50 important words.\n",
    "print(\"Top 50 words in order of importance:\")\n",
    "for importance, term in top_50_features:\n",
    "    print(f\"Word: {term}, Importance: {importance:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ff486-0bb2-4408-9c43-18a923b3457a",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff175f3e-bfaa-49d1-aa57-43e5c8d174ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Word2Vec and text preprocessing.\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess  # For preprocessing text into tokens.\n",
    "from nltk.corpus import stopwords  # For accessing stopwords.\n",
    "import nltk  # For additional NLP utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a4b6578-799a-4e8a-816f-e0a70979bb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lila9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the NLTK stopwords dataset and create a set of English stopwords for faster lookups.\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b80684-83f8-4357-b431-9c44cb37e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess text by tokenizing and removing stopwords.\n",
    "def preprocess_text(text):\n",
    "    tokens = simple_preprocess(text) # Tokenize the text into lowercase words.\n",
    "    return [word for word in tokens if word not in stop_words] # Filter out stopwords from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38731b-3e81-4a8e-b807-2cf6d21a3c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv('df_combined.csv')\n",
    "corpus = df_combined['Content']\n",
    "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2453e-d277-4f29-a2c6-2e34e83ba8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec Model\n",
    "model = Word2Vec(\n",
    "    sentences=preprocessed_corpus,  # Preprocessed sentences\n",
    "    vector_size=100,                # Size of word vectors\n",
    "    window=5,                       # Context window size\n",
    "    min_count=2,                    # Minimum word frequency\n",
    "    workers=4,                      # Number of threads\n",
    "    sg=1                            # Skip-gram model (set 0 for CBOW)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b5c0c-fe60-47ef-ac53-231421ba1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to ensure reusability\n",
    "model_path = \"word2vec_model.model\"\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4197a1-3062-4a4a-b036-0f0ad6983e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Accessing a word vector\n",
    "word = \"accept\" \n",
    "if word in model.wv:\n",
    "    print(f\"Vector for '{word}': {model.wv[word]}\")\n",
    "\n",
    "# Example: Finding similar words\n",
    "try:\n",
    "    similar_words = model.wv.most_similar(word, topn=10)\n",
    "    print(f\"Words similar to '{word}': {similar_words}\")\n",
    "except KeyError:\n",
    "    print(f\"Word '{word}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e04f52-2099-40fb-8f55-0e295066d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Accessing a word vector\n",
    "word = \"reject\" \n",
    "if word in model.wv:\n",
    "    print(f\"Vector for '{word}': {model.wv[word]}\")\n",
    "\n",
    "# Example: Finding similar words\n",
    "try:\n",
    "    similar_words = model.wv.most_similar(word, topn=10)\n",
    "    print(f\"Words similar to '{word}': {similar_words}\")\n",
    "except KeyError:\n",
    "    print(f\"Word '{word}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211c677-8200-495c-9202-3f3d07e01ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Laden der CSV-Daten\n",
    "data = pd.read_csv('df.csv')\n",
    "\n",
    "# Erste Ansicht der Daten\n",
    "print(data.head())\n",
    "\n",
    "# Berechnung neuer Features\n",
    "# Wortanzahl der Nachricht\n",
    "data['Word_Count'] = data['Content'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Zeichenanzahl der Nachricht\n",
    "data['Char_Length'] = data['Content'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Durchschnittliche Wortlänge\n",
    "data['Avg_Word_Length'] = data['Content'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\n",
    "\n",
    "# Erfolgsspalte (Zielvariable)\n",
    "y = data['NegoOutcomeLabel']\n",
    "\n",
    "# Features für die Analyse\n",
    "X = data[['Word_Count', 'Char_Length', 'Avg_Word_Length']]\n",
    "\n",
    "# Datenaufteilung in Training und Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Forest Modell zur Erkennung von Zusammenhängen\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen und Bewertung\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Feature Importance\n",
    "importances = rf.feature_importances_\n",
    "features = X.columns\n",
    "plt.bar(features, importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "# Analyse der Wortinhalte mit CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=50)\n",
    "X_vectorized = vectorizer.fit_transform(data['Content'].fillna('')).toarray()\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Hinzufügen der häufigsten Wörter als Features\n",
    "for idx, word in enumerate(feature_names):\n",
    "    data[word] = X_vectorized[:, idx]\n",
    "\n",
    "# Aktualisierte Features für die zweite Analyse\n",
    "X_updated = data[['Word_Count', 'Char_Length', 'Avg_Word_Length'] + list(feature_names)]\n",
    "\n",
    "# Erneute Aufteilung in Training und Test\n",
    "X_train_updated, X_test_updated, y_train_updated, y_test_updated = train_test_split(X_updated, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Neues Modell mit erweiterten Features\n",
    "rf_updated = RandomForestClassifier(random_state=42)\n",
    "rf_updated.fit(X_train_updated, y_train_updated)\n",
    "\n",
    "# Neue Vorhersagen und Bewertung\n",
    "y_pred_updated = rf_updated.predict(X_test_updated)\n",
    "print(\"Updated Classification Report:\")\n",
    "print(classification_report(y_test_updated, y_pred_updated))\n",
    "print(\"Updated Accuracy Score:\", accuracy_score(y_test_updated, y_pred_updated))\n",
    "\n",
    "\n",
    "# Neue Feature Importance Visualisierung\n",
    "updated_importances = rf_updated.feature_importances_\n",
    "updated_features = X_updated.columns\n",
    "\n",
    "# Nur die wichtigsten 10 Features anzeigen\n",
    "sorted_indices = np.argsort(updated_importances)[-10:]\n",
    "plt.bar(np.array(updated_features)[sorted_indices], updated_importances[sorted_indices])\n",
    "plt.title('Top 10 Updated Feature Importances')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16baf818-8fe0-4937-9159-7237554b3334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
