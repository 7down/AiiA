{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf353bf-1f73-45db-ae99-ab510788b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, TFDistilBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.data import Dataset\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80684e1b-1387-4b23-9635-11ccc80bf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('df.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490dd15-da22-481f-84c9-00676044425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization to the 'Content' column\n",
    "df['Tokenized_Content'] = df['Content'].apply(tokenize_text)\n",
    "\n",
    "# Perform inference using the model\n",
    "def get_embeddings(tokenized_text):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_text)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Apply the model to get embeddings for each tokenized text\n",
    "df['Embeddings'] = df['Tokenized_Content'].apply(get_embeddings)\n",
    "\n",
    "# Save the DataFrame to a pickle file\n",
    "with open('df_with_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file (without embeddings)\n",
    "df.drop(columns=['Tokenized_Content', 'Embeddings']).to_csv('df_with_embeddings.csv', index=False)\n",
    "\n",
    "# Display the DataFrame with embeddings\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffee64-40f9-4b2f-8f33-3566b6df76d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for RandomForestClassifier\n",
    "print(\"RandomForestClassifier with Oversampling:\")\n",
    "smote = SMOTE(random_state=42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Extract embeddings and target variable\n",
    "X = np.vstack(df['Embeddings'])\n",
    "y = df['outcome']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate RandomForestClassifier\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be78e6-44a8-4f0a-9daf-4c59f4c73423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance using SHAP\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test[:100])\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.title(\"Feature Importance for RandomForest\")\n",
    "shap.summary_plot(shap_values, X_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1b8e0-bf3e-400d-aaf8-23daae6dabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilBERT Model Training\n",
    "print(\"Training DistilBERT Model:\")\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "history_list = []\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X, y):\n",
    "    print(f\"Starting fold {fold}...\")\n",
    "\n",
    "    # Split the data for this fold\n",
    "    train_embeddings, val_embeddings = X[train_index], X[val_index]\n",
    "    train_labels, val_labels = y[train_index], y[val_index]\n",
    "\n",
    "    # Create TensorFlow datasets for training and validation\n",
    "    train_dataset = Dataset.from_tensor_slices((train_embeddings, train_labels)).shuffle(len(train_embeddings)).batch(16)\n",
    "    val_dataset = Dataset.from_tensor_slices((val_embeddings, val_labels)).batch(16)\n",
    "\n",
    "    # Load the DistilBERT model for sequence classification\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(np.unique(y)))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=5e-5)\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    # Set up early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=5,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    history_list.append(history)\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee87130-c9c8-4cc1-bd55-820a1906c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate overfitting by plotting training vs. validation accuracy for all folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, history in enumerate(history_list):\n",
    "    plt.plot(history.history['accuracy'], label=f'Fold {i+1} Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label=f'Fold {i+1} Validation Accuracy', linestyle='--')\n",
    "plt.title('Training vs. Validation Accuracy Across Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f2234-d766-49b8-9160-bb122944d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using attention scores\n",
    "# Extract attention scores from the model (example-based visualization)\n",
    "def visualize_attention(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\")\n",
    "    outputs = model(inputs, output_attentions=True)\n",
    "    attention = outputs.attentions[-1][0]  # Last layer attention\n",
    "    attention_weights = tf.reduce_mean(attention, axis=1).numpy()\n",
    "\n",
    "    print(\"Attention Weights:\")\n",
    "    for token, weight in zip(tokenizer.tokenize(text), attention_weights.flatten()):\n",
    "        print(f\"{token}: {weight:.4f}\")\n",
    "\n",
    "example_text = \"Example negotiation message to visualize feature importance.\"\n",
    "visualize_attention(model, tokenizer, example_text)\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"distilbert_negotiation_model\")\n",
    "tokenizer.save_pretrained(\"distilbert_negotiation_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db323b6-c507-466c-a491-997f6f81a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results\n",
    "# Decode predictions for interpretation\n",
    "predictions = model.predict(val_dataset).logits\n",
    "predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
    "\n",
    "# Create a DataFrame to compare predictions with actual outcomes\n",
    "results_df = pd.DataFrame({\n",
    "    'message': val_embeddings,\n",
    "    'actual_outcome': val_labels,\n",
    "    'predicted_outcome': predicted_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377ba5c-6db0-4772-83ea-8a7f9060b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results for further analysis\n",
    "results_df.to_csv(\"negotiation_results.csv\", index=False)\n",
    "\n",
    "print(\"Results saved to 'negotiation_results.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
